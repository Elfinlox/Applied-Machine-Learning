{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc57c32",
   "metadata": {},
   "source": [
    "# Assignment 1 - Applied Machine Learning\n",
    "---\n",
    "## Arghadeep Ghosh\n",
    "\n",
    "This notebook 'train.csv' contains the code for loading the Training, Validation and Test datasets, fitting Naive-Bayes, Logistic Regression and Random Forest models on the training data and evaluating the model on the Validation and Test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5953b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acd3e540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3344, 1115, 1115)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('validation.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287dd9be",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "---\n",
    "The data is converted from a sentence model to a Bag of words format with each word asigned a tf-idf weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3eefbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [xmas, iscoming, ur, awarded, either, £500, cd...\n",
       "1                 [noice, text, me, when, you, 're, here]\n",
       "2                    [85233, free, ringtone, reply, real]\n",
       "3       [sorry, sir, i, will, call, you, tomorrow, sen...\n",
       "4       [u, 447801259231, have, a, secret, admirer, wh...\n",
       "                              ...                        \n",
       "3339                   [i, am, in, a, marriage, function]\n",
       "3340    [a, £400, xmas, reward, is, waiting, for, you,...\n",
       "3341    [can, you, tell, shola, to, please, go, to, co...\n",
       "3342                    [sir, waiting, for, your, letter]\n",
       "3343    [message, from, i, am, at, truro, hospital, on...\n",
       "Name: message, Length: 3344, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_into_lemmas(message):\n",
    "    message = message.lower()  # convert bytes into proper unicode\n",
    "    words = TextBlob(message).words\n",
    "    return [word.lemma for word in words]\n",
    "\n",
    "train.message.apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "037b9a0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CountVectorizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m word_vec \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 2\u001b[0m bow_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mword_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_into_lemmas\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m train_bow \u001b[38;5;241m=\u001b[39m bow_transformer\u001b[38;5;241m.\u001b[39mtransform(train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m valid_bow \u001b[38;5;241m=\u001b[39m bow_transformer\u001b[38;5;241m.\u001b[39mtransform(valid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'CountVectorizer' object is not callable"
     ]
    }
   ],
   "source": [
    "word_vec = CountVectorizer(analyzer=split_into_lemmas)\n",
    "word_vec.fit(train['message'])\n",
    "train_bow = word_vec.transform(train['message'])\n",
    "valid_bow = word_vec.transform(valid['message'])\n",
    "test_bow = word_vec.transform(test['message'])\n",
    "\n",
    "bow = word_vec.transform([train['message'][5]])\n",
    "\n",
    "print(word_vec.get_feature_names_out()[2096])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(train_bow)\n",
    "train_tfidf = tfidf_transformer.transform(train_bow)\n",
    "valid_tfidf = tfidf_transformer.transform(valid_bow)\n",
    "test_tfidf = tfidf_transformer.transform(test_bow)\n",
    "\n",
    "\n",
    "tfidf = tfidf_transformer.transform(bow)\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, Y):\n",
    "    Y_pred = model.predict(X)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(Y, Y_pred)*100, '%')\n",
    "    print('Precision:', precision_score(Y, Y_pred, pos_label = 'spam')*100, '%')\n",
    "    print('Recall:', recall_score(Y, Y_pred, pos_label = 'spam')*100, '%')\n",
    "    print('F1 Score:', f1_score(Y, Y_pred, pos_label = 'spam')*100, '%')\n",
    "\n",
    "    cm = confusion_matrix(Y, Y_pred)\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    ax.xaxis.set_ticklabels(['Not Spam', 'Spam']); ax.yaxis.set_ticklabels(['Not Spam', 'Spam']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d859286",
   "metadata": {},
   "source": [
    "## Naive-Bayes Model\n",
    "---\n",
    "We train a Multinomial Naive-Bayes model on the training data and evaluate it on the validation and testing data. We've obtained the Accuracy, precision, recall, F1 score and the confusion matrix in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b899e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spam_detectorNB = MultinomialNB().fit(train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0ef63",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorNB, train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5fad5",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798059f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorNB, valid_tfidf, valid['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1673d",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37759a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorNB, test_tfidf, test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc8e85",
   "metadata": {},
   "source": [
    "## Logistic Regression Model\n",
    "---\n",
    "We train a Logistic Regression model on the training data and evaluate it on the validation and testing data. We've obtained the Accuracy, precision, recall, F1 score and the confusion matrix in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4700c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spam_detectorLR = LogisticRegression().fit(train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a432e",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorLR, train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a66fb",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d239a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorLR, valid_tfidf, valid['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806a4fa7",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorLR, test_tfidf, test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f863c",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "---\n",
    "We train a Random Forest Classifier model on the training data and evaluate it on the validation and testing data. We've obtained the Accuracy, precision, recall, F1 score and the confusion matrix in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f62ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spam_detectorRF = RandomForestClassifier().fit(train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5a626",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorRF, train_tfidf, train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f1bbc1",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf79d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorRF, valid_tfidf, valid['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ea7e3",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(spam_detectorRF, valid_tfidf, valid['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe364f0f",
   "metadata": {},
   "source": [
    "## The best model\n",
    "---\n",
    "Upon observing the scores achieved on the testing data for the three models we can observe that the Random Forest Classifier performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2a94029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os\n",
    "\n",
    "if not os.path.exists(\"../Assignment 3/models\"):\n",
    "    os.mkdir(\"../Assignment 3/models\")\n",
    "\n",
    "vec_path = \"../Assignment 3/models/word_vec.sav\"\n",
    "tfidf_path = \"../Assignment 3/models/tfidf.sav\"\n",
    "nb_path = \"../Assignment 3/models/nb_model.sav\"\n",
    "lr_path = \"../Assignment 3/models/lr_model.sav\"\n",
    "rf_path = \"../Assignment 3/models/rf_model.sav\"\n",
    "\n",
    "pickle.dump(word_vec, open(vec_path, \"wb\"))\n",
    "pickle.dump(tfidf_transformer, open(tfidf_path, \"wb\"))\n",
    "pickle.dump(spam_detectorNB, open(nb_path, \"wb\"))\n",
    "pickle.dump(spam_detectorLR, open(lr_path, \"wb\"))\n",
    "pickle.dump(spam_detectorRF, open(rf_path, \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
